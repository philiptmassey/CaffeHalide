SUMMARY ==========================================

We implemented a convolutional neural network (LeNet) in Halide, compared correctness with a Theano (Python) implementation, and raced the industry standard neural network software (Caffe) CPU implementation.
We demonstrated that our Halide implementation can achieve performance within 14\% of the baseline Caffe implementation on a 12-core Intel Xeon CPU.

BACKGROUND =======================================

We will first describe a neural network at a high level, since this concept is crucial to the project.
A neural network, for our purposes, is a machine learning structure that can be drawn as a directed layer graph, where each layer of nodes takes input from the layer below and outputs to the layer above.
We will use this structure to classify images, and the final layer will be a maximum over probabilities of the input to be in a certain class.
Additionally, each "hidden" node of the network (an interior node that is not located in the bottom-most or top-most layer) performs an activation on the sum of its inputs multiplied by its parameters.
The activation is commonly chosen to be the hyperbolic tangent function, for a reason beyond the scope of this write-up.
We will sometimes refer to these hidden layers as "convolution" layers, as the size of the weight filter is not generally equal to the size of the image, so we must convolve the input with the weight matrices.
A neural network is trained by running images through the network, calculating the loss function, and then calculating gradients (for gradient descent) by sending information backward through the network.  
This is commonly known as backpropagation.  
The loss function can be varied, and is not crucial to understand, but essentially respresents how far from the optimal parameters we are.
The backpropagation algorithm uses this loss function (which we only know after evaluating the network on training data) to compute what direction we should change our parameters (i.e. whether to decrease, increase, or remain roughly the same).
This direction (and magnitude) is just the gradient of the loss function with respect to the parameters, which is why the training algorithm for individual nodes is called gradient descent.
As a last point, the Caffe learning framework is currently the industry standard for training and evaluating neural networks; the Theano framework is another that is easier to modify and understand, since it is in Python.

In addition to neural networks, out project involves a domain-specific language known as Halide, developed at MIT.
Here we will give a brief overview of Halide's features.
Halide is a JIT-compiled language for image processing and pipelining that offers high level abstractions for programmers to implement powerful low level optimizations.
Halide splits code into two sections: the first portion of any Halide program defines the algorithm to be executed, and the second portion (optional) defines the schedule on which to execute the algorithm.
Halide algorithms define images via functions by pixel-by-pixel definitions.
These transformative functions take an image and an input and give an image as an output.
We can define a pipeline of these functions in order to transform the input image into the class the network classifies it as.
The scheduling portion of Halide code is where major optimizations such as tiling or parallelizing can be defined.
The image processing abstractions such as tiling, fusing, parallelizing, and others are optimized by Halide's compiler for the particular system on which Halide is being run, which allows development of ultra-portable code that optimizes itself intelligently for a given architecture.

To conclude the background section we will discuss an implementation of LeNet, a certain neural network, in Halide.
LeNet, named after Yann LeCunn, is made up of two convolution layers (which are also max-pooled, that is to say we subsample the convolved matrix using a simple max function to reduce image size), a fully connected layer (which is simple matrix multiplications of the previous layer's output), and a softmax layer (which actually gives us the predicted class).
LeNet has been proven to work well on the MNIST data set, which is a collection of hand-drawn numbers (from 0 to 9).
The classification task therefore has 10 natural classes, and LeNet has demonstrated >95% accuracy on MNIST with just 4 nodes in the first convolution layer and 6 in the second.

APPROACH =========================================

Our approach used the Halide language, the Caffe learning framework, the Theano learning framework, and the Latedays cluster.
We used an Intel Xeon 12-core CPU on the Latedays clusters to perform our computation.
In implementing LeNet, we did not need to change the general serial algorithm.
In order to introduce parallelism, we allowed for multiple images to be classified at the same time, and noting portions of the layer computations that could be done simultaneously.
The coding of the algorithm in Halide did not require many iterations, since most of the structure had already been clearly outlined in documentation of LeNet and Theano.

Our implemention of LeNet in Halide started with mapping the concept of neural network layers to Halide semantics.
In order to do this, we need to define the data, which is a Halide image that travels through the neural network, and the layers, which are Halide functions that transform the image as it travels.
The data is a 4D image, where the dimensions are the x and y pixel coordinate, the feature map number, and the image number.
By defining the data as this kind of image, we will be able to exploit many kinds of inherent parallelism, as well as easily perform the matrix operations neccessary for classification.
The layers are Halide functions that take in the data image as input (as well as the weight image, bias image, and other neccessary parameters) an perform a predefined operation on the data.
By defining layers in this way, each layer only needs to perform a simple calculation.
The majority of the layers can represent their operation in one line of Halide code, as the inclusion of reduction domains allow one output pixel to depend on multiple input pixels.

A neural network consists of multiple layers acting upon a input, creating the classification output.
Using the data image and layer functions, we defined our neural network in Halide by pipelining multiple layers together to act upon a the input image.

In defining our neural network in this way, we expose many layers of parallelism, which we utilized through scheduling in Halide.
The first and most important is also the most obvious.
Each input image in the testing set is complete independent of the other input images as their travel through the network.
Because the image data is defined as a 4D image, the fourth dimension can be done completely in parallel throughout the entire network.
This was done using Halide's ``.parallel()" notation for the layers.
Halide uses a thread worker pool, so it's smart to create many more instances of work than there are workers (to balance the scheduling overhead).
This approach also speeds up computation by using locality, because each worker only needs to focus on a single (or batch) of images.
Because we were on a 12-core processor with hyperthreading, we used a pool with 24 workers.

Another, less obvious parallelism in our network is on the layer level.
Each layer is only dependent on the layer before it, and within each layer each pixel is completely independent of the other pixels.
Therefore, we can using SIMD vector instructions through the ``.vectorize()" notation, combined with tiling and loop unrolling through the ``.tile()" and ``.unroll()" instructions to improve runtime.
We emperically found that 4-wide vector instructions performed best.

One feature in Halide is it's JIT computation.
Halide will only compute the values of a pixel if the final image is depenent on it.
However, as Halide does its computation, it may perform different pieces of different layers out of order.
This means that if certain output pixels are dependent on the same input pixels, there is a chance that it will be computed multiple times.
By using the ``.compute_root()" notation, we assure that each layer is fully computed before the next layer is started.
While this may seem to hurt computation, because at each pixel at each layer the same amount of work is done, the removal of any repetitive work significantly speeds up our program.

RESULTS ====================

We were very happy with our results for the LeNet convolution neural network evaluation times.

\begin{figure}
\incudegraphix{graph.png}
\caption{Graph}
\end{figure}

\begin{figure}
\centering
\begin{tabular}{| c | c |}
Number of Images to Classify & Classification Time (milliseconds / image) \\ \hline
10 & 54.74 \\ \hline
50 & 11.55 \\ \hline
100 & 6.11 \\ \hline
500 & 1.77 \\ \hline
1000 & 1.22 \\ \hline
5000 & 0.79 \\ \hline
10000 & 0.74 \\ \hline
50000 & 0.69 \\ \hline
\end{tabular}
\caption{Halide classification times for different testing set sizes.}
\end{figure}

We chose not to explore GPU evaluation on the NVIDIA K40 GPUs on the Latedays cluster, as we believed we had a better chance of out-performing Caffe's CPU implementation.

REFERENCES =======================

